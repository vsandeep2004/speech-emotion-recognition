The workflow starts by traversing the directory containing the TESS Toronto emotional speech dataset to extract file paths and corresponding labels from the filenames. This process ensures that the paths and labels are accurately paired and organized for further processing. The data is then structured into a pandas DataFrame, allowing for easy manipulation and analysis. The distribution of different emotional labels is visualized using a count plot, providing insights into the balance of the dataset and potential biases.
To understand the characteristics of the audio data, visualization functions are defined for generating waveforms and spectrograms. The waveplot function produces a time-domain representation of the audio signal, while the spectogram function provides a time-frequency representation. These visualizations are crucial for differentiating the variations between emotional states. The script applies these functions to several samples, displaying the waveforms and spectrograms for various emotions such as fear, anger, disgust, neutral, sadness, surprise (ps), and happiness.
For feature extraction, Mel-frequency cepstral coefficients (MFCCs) are computed from the audio files, capturing the timbral aspects of the speech relevant for emotion classification. The extracted MFCC features are compiled into an array and reshaped for compatibility with the neural network model. An LSTM (Long Short-Term Memory) neural network is constructed with layers for sequence learning, regularization, and classification. The model is trained using categorical cross-entropy loss and the Adam optimizer, with training accuracy and loss tracked over five epochs. Finally, the performance is evaluated by plotting the accuracy and loss metrics for both training and validation sets, highlighting the model's effectiveness in emotion classification.
